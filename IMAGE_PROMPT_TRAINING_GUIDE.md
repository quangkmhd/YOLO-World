# HÆ°á»›ng dáº«n Training Image Prompt YOLO-World

## ğŸ¯ Tá»•ng quan Image Prompt Training

Image Prompt Training lÃ  phÆ°Æ¡ng phÃ¡p training YOLO-World Ä‘á»ƒ cÃ³ thá»ƒ detect objects báº±ng cÃ¡ch sá»­ dá»¥ng cáº£ **text prompts** vÃ  **image prompts** (áº£nh máº«u cá»§a object cáº§n detect).

## ğŸ“‹ Quy trÃ¬nh Training tá»«ng bÆ°á»›c

### BÆ°á»›c 1: Chuáº©n bá»‹ mÃ´i trÆ°á»ng vÃ  dependencies
```bash
# Setup YOLO-World environment
conda create -n yoloworld python=3.8
conda activate yoloworld
pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu121
pip install openmim
mim install mmcv==2.1.0
pip install -e .
```

**Files cáº§n thiáº¿t:**
- `setup_yoloworld.sh` - Script tá»± Ä‘á»™ng setup
- `requirements/basic_requirements.txt` - Dependencies cÆ¡ báº£n
- `pyproject.toml` - Package configuration

### BÆ°á»›c 2: Táº£i Pre-trained Models
```bash
# Táº£i YOLO-World base model (Ä‘Ã£ pre-train)
wget -O pretrained_models/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth \
  "https://huggingface.co/wondervictor/YOLO-World-V2.1/resolve/main/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth"

# Táº£i CLIP model cho text/image encoding
mkdir -p pretrained_models/open-ai-clip-vit-base-patch32
# ... (download CLIP files)
```

**Files cáº§n thiáº¿t:**
- `yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth` - Base YOLO-World model
- `pretrained_models/open-ai-clip-vit-base-patch32/` - CLIP model files
- `download_models_for_image_prompt.py` - Script táº£i models

### BÆ°á»›c 3: Chuáº©n bá»‹ Dataset
```bash
# Cáº¥u trÃºc dataset cho image prompt training
data/
â”œâ”€â”€ coco/
â”‚   â”œâ”€â”€ train2017/          # Training images
â”‚   â”œâ”€â”€ val2017/            # Validation images
â”‚   â””â”€â”€ annotations/
â”‚       â”œâ”€â”€ instances_train2017.json
â”‚       â””â”€â”€ instances_val2017.json
â””â”€â”€ texts/
    â””â”€â”€ coco_class_texts.json  # Text descriptions cho classes
```

**Files cáº§n thiáº¿t:**
- `data/texts/coco_class_texts.json` - Text prompts cho cÃ¡c class
- Dataset images vÃ  annotations
- Custom dataset converter (náº¿u dÃ¹ng custom data)

### BÆ°á»›c 4: Cáº¥u hÃ¬nh Training Config
**File chÃ­nh:** `configs/image_prompts/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_image_prompt_demo.py`

**CÃ¡c thÃ nh pháº§n quan trá»ng:**
```python
# Base model Ä‘á»ƒ load weights
load_from = 'pretrained_models/yolo_world_l_clip_t2i_bn_2e-3adamw_32xb16-100e_obj365v1_goldg_cc3mlite_train-ca93cd1f.pth'

# CLIP model cho text/image encoding (FROZEN)
text_model_name = '../pretrained_models/open-ai-clip-vit-base-patch32'

# Model architecture
model = dict(
    type='YOLOWorldImageDetector',  # â† Image prompt detector
    vision_model=text_model_name,   # â† Sá»­ dá»¥ng CLIP vision
    backbone=dict(
        text_model=dict(
            type='HuggingCLIPLanguageBackbone',
            frozen_modules=['all']  # â† CLIP bá»‹ Ä‘Ã³ng bÄƒng
        )
    ),
    neck=dict(freeze_all=True),     # â† Neck bá»‹ Ä‘Ã³ng bÄƒng
    bbox_head=dict(freeze_all=True) # â† Head bá»‹ Ä‘Ã³ng bÄƒng
)
```

### BÆ°á»›c 5: Cháº¡y Training
```bash
# Single GPU training
python tools/train.py configs/image_prompts/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_image_prompt_demo.py

# Multi-GPU training (8 GPUs)
./tools/dist_train.sh configs/image_prompts/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_image_prompt_demo.py 8 --amp

# Custom work directory
python tools/train.py configs/image_prompts/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_image_prompt_demo.py \
    --work-dir work_dirs/image_prompt_training
```

**Files sá»­ dá»¥ng:**
- `tools/train.py` - Main training script
- `tools/dist_train.sh` - Distributed training script
- Config file - Training configuration

### BÆ°á»›c 6: Monitoring vÃ  Evaluation
```bash
# Theo dÃµi training progress
tensorboard --logdir work_dirs/image_prompt_training

# Evaluation trÃªn validation set
python tools/test.py configs/image_prompts/yolo_world_v2_l_vlpan_bn_2e-4_80e_8gpus_image_prompt_demo.py \
    work_dirs/image_prompt_training/latest.pth \
    --out results.pkl
```

**Files Ä‘Æ°á»£c táº¡o:**
- `work_dirs/image_prompt_training/` - Training outputs
- `latest.pth`, `best.pth` - Model checkpoints
- `scalars.json` - Training metrics
- `results.pkl` - Evaluation results

## ğŸ”§ Chi tiáº¿t tá»«ng thÃ nh pháº§n

### 1. **YOLOWorldImageDetector**
- **Má»¥c Ä‘Ã­ch:** Detector há»— trá»£ cáº£ text vÃ  image prompts
- **Input:** Images + text descriptions + image examples
- **Output:** Bounding boxes + class predictions

### 2. **CLIP Model (Frozen)**
- **Má»¥c Ä‘Ã­ch:** Extract semantic features tá»« text vÃ  images
- **Vai trÃ²:** Feature extractor cá»‘ Ä‘á»‹nh, khÃ´ng Ä‘Æ°á»£c training
- **LÃ½ do freeze:** Giá»¯ nguyÃªn pre-trained semantic knowledge

### 3. **MultiModalYOLOBackbone**
- **Má»¥c Ä‘Ã­ch:** Káº¿t há»£p CLIP features vá»›i YOLO detection
- **Components:**
  - `image_model`: YOLO backbone cho image features
  - `text_model`: CLIP text encoder (frozen)
  - `frozen_stages`: Freeze má»™t sá»‘ layer cá»§a image model

### 4. **Training Strategy**
```python
# Chá»‰ train cÃ¡c layer alignment
optim_wrapper = dict(
    paramwise_cfg=dict(
        custom_keys={
            'backbone.text_model': dict(lr_mult=0.01),  # CLIP lr ráº¥t nhá»
            'logit_scale': dict(weight_decay=0.0),
            'embeddings': dict(weight_decay=0.0)
        }
    )
)
```

## ğŸ“Š Luá»“ng dá»¯ liá»‡u trong Training

```
Input Images â†’ YOLO Backbone â†’ Image Features
                    â†“
Text Prompts â†’ CLIP Text Encoder â†’ Text Features
                    â†“
Image Examples â†’ CLIP Vision Encoder â†’ Visual Features
                    â†“
            Multimodal Fusion
                    â†“
            Detection Head â†’ Predictions
                    â†“
            Loss Calculation â†’ Backprop
```

## ğŸ¯ Káº¿t quáº£ Training

**Model sau training cÃ³ thá»ƒ:**
1. Detect objects báº±ng text descriptions
2. Detect objects báº±ng image examples
3. Zero-shot detection trÃªn classes má»›i
4. Few-shot learning vá»›i Ã­t examples

**Files output:**
- `best.pth` - Model weights tá»‘t nháº¥t
- `config.py` - Training configuration
- `scalars.json` - Training metrics
- `log.txt` - Training logs

## ğŸ’¡ Tips Training hiá»‡u quáº£

1. **Memory Management:**
   - Sá»­ dá»¥ng `--amp` cho mixed precision
   - Giáº£m batch size náº¿u OOM
   - Freeze nhiá»u layers Ä‘á»ƒ tiáº¿t kiá»‡m memory

2. **Data Preparation:**
   - Chuáº©n bá»‹ text descriptions cháº¥t lÆ°á»£ng cao
   - Sá»­ dá»¥ng diverse image examples
   - Balance dataset giá»¯a cÃ¡c classes

3. **Hyperparameter Tuning:**
   - Learning rate tháº¥p cho frozen components
   - Warmup scheduler cho stable training
   - Early stopping Ä‘á»ƒ trÃ¡nh overfitting

## ğŸš€ Sá»­ dá»¥ng Model sau Training

```python
# Load trained model
from yolo_world import YOLOWorldImageDetector
model = YOLOWorldImageDetector.from_pretrained('work_dirs/image_prompt_training/best.pth')

# Inference vá»›i text prompt
results = model.predict(image, text_prompts=["person", "car"])

# Inference vá»›i image prompt
results = model.predict(image, image_prompts=[example_person_img, example_car_img])
```

Quy trÃ¬nh nÃ y cho phÃ©p training YOLO-World Ä‘á»ƒ cÃ³ kháº£ nÄƒng detect objects báº±ng cáº£ text vÃ  image prompts, táº­n dá»¥ng sá»©c máº¡nh cá»§a CLIP Ä‘á»ƒ hiá»ƒu semantic content.
